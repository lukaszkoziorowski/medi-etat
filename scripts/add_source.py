#!/usr/bin/env python3
"""
CLI tool for adding new scraping sources with AI-assisted detection.
Usage: python scripts/add_source.py <url> [--city CITY] [--name NAME]
"""
import sys
import os
import re
from urllib.parse import urlparse

# Add backend to path
backend_path = os.path.join(os.path.dirname(__file__), '..', 'backend')
sys.path.insert(0, backend_path)

# Change to backend directory
os.chdir(backend_path)

from app.scrapers.detector import StructureDetector
from app.scrapers.config_loader import SourceConfig, save_config, get_configs_dir
from app.scrapers.base import BaseScraper
from app.scrapers.config_scraper import ConfigBasedScraper


def generate_source_id(url: str, name: str = None) -> str:
    """Generate a source ID from URL or name."""
    if name:
        # Convert to lowercase, replace spaces with underscores, remove special chars
        source_id = re.sub(r'[^a-z0-9_]', '', name.lower().replace(' ', '_'))
        return source_id
    
    # Extract from URL
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '').split('.')[0]
    path = parsed.path.strip('/').replace('/', '_')
    source_id = f"{domain}_{path}" if path else domain
    source_id = re.sub(r'[^a-z0-9_]', '', source_id)
    return source_id or 'source'


def detect_structure(url: str, use_playwright: bool = False) -> dict:
    """Detect structure of a job listing page."""
    print(f"Fetching page: {url}")
    
    # Create a temporary scraper to fetch the page
    class TempScraper(BaseScraper):
        def scrape(self):
            return []
    
    scraper = TempScraper(url, "Temp", "Gdańsk")
    soup = scraper.fetch_page(url, use_playwright=use_playwright)
    
    if not soup:
        print("Error: Could not fetch page. Trying with Playwright...")
        if not use_playwright:
            soup = scraper.fetch_page(url, use_playwright=True)
            if soup:
                use_playwright = True
    
    if not soup:
        print("Error: Failed to fetch page even with Playwright.")
        return None
    
    print("Analyzing page structure...")
    detector = StructureDetector(soup, url)
    result = detector.detect()
    
    result['requiresPlaywright'] = use_playwright
    return result


def create_config(url: str, detection_result: dict, city: str = "Gdańsk", source_name: str = None) -> SourceConfig:
    """Create a source config from detection results."""
    if not source_name:
        parsed = urlparse(url)
        source_name = parsed.netloc.replace('www.', '')
    
    source_id = generate_source_id(url, source_name)
    
    config_dict = {
        "sourceId": source_id,
        "sourceName": source_name,
        "baseUrl": url,
        "city": city,
        "facilityName": source_name,
        "selectors": {
            "jobListContainer": detection_result.get('jobListContainer'),
            "jobItem": detection_result.get('jobItem'),
            "title": detection_result.get('title'),
            "link": detection_result.get('link'),
            "description": detection_result.get('description'),
        },
        "extraction": {
            "titleFrom": "text",
            "linkFrom": "href",
            "normalizeLink": True
        },
        "pagination": {
            "type": "none",
            "param": "page",
            "maxPages": 10
        },
        "metadata": {
            "confidence": detection_result.get('confidence', 'LOW'),
            "autoGenerated": True,
            "verifiedBy": "system",
            "notes": f"Auto-detected. {', '.join(detection_result.get('reasoning', []))}",
            "requiresPlaywright": detection_result.get('requiresPlaywright', False)
        }
    }
    
    return SourceConfig(config_dict)


def preview_config(config: SourceConfig, detection_result: dict):
    """Preview what will be scraped with this config."""
    print("\n" + "=" * 60)
    print("CONFIGURATION PREVIEW")
    print("=" * 60)
    
    print(f"\nSource: {config.source_name}")
    print(f"URL: {config.base_url}")
    print(f"City: {config.city}")
    print(f"Confidence: {detection_result.get('confidence', 'LOW')}")
    
    print(f"\nSelectors:")
    print(f"  Job List Container: {config.job_list_container or 'N/A'}")
    print(f"  Job Item: {config.job_item or 'N/A'}")
    print(f"  Title: {config.title_selector or 'N/A'}")
    print(f"  Link: {config.link_selector or 'N/A'}")
    
    if detection_result.get('reasoning'):
        print(f"\nDetection Reasoning:")
        for reason in detection_result.get('reasoning', []):
            print(f"  - {reason}")
    
    if detection_result.get('sampleJobs'):
        print(f"\nSample Jobs Found ({len(detection_result['sampleJobs'])}):")
        for i, job in enumerate(detection_result['sampleJobs'][:5], 1):
            print(f"  {i}. {job['title'][:60]}...")
            print(f"     URL: {job['url'][:60]}...")
    
    print("\n" + "=" * 60)


def test_config(config: SourceConfig):
    """Test the config by running a scrape."""
    print("\nTesting configuration...")
    scraper = ConfigBasedScraper(config)
    jobs = scraper.scrape()
    
    print(f"Found {len(jobs)} job offers")
    
    if jobs:
        print("\nSample scraped jobs:")
        for i, job in enumerate(jobs[:3], 1):
            print(f"  {i}. {job['title'][:60]}...")
            print(f"     Role: {job['role'].value}")
            print(f"     URL: {job['source_url'][:60]}...")
    
    return len(jobs) > 0


def main():
    """Main CLI function."""
    if len(sys.argv) < 2:
        print("Usage: python scripts/add_source.py <url> [--city CITY] [--name NAME] [--playwright]")
        print("\nExample:")
        print("  python scripts/add_source.py https://szpital.pl/kariera --city Warszawa --name 'Szpital XYZ'")
        sys.exit(1)
    
    url = sys.argv[1]
    city = "Gdańsk"
    source_name = None
    use_playwright = False
    
    # Parse arguments
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == '--city' and i + 1 < len(sys.argv):
            city = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--name' and i + 1 < len(sys.argv):
            source_name = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--playwright':
            use_playwright = True
            i += 1
        else:
            i += 1
    
    print("=" * 60)
    print("AI-ASSISTED SOURCE ONBOARDING")
    print("=" * 60)
    
    # Step 1: Detect structure
    detection_result = detect_structure(url, use_playwright)
    if not detection_result:
        print("Failed to detect structure. Exiting.")
        sys.exit(1)
    
    # Step 2: Create config
    config = create_config(url, detection_result, city, source_name)
    
    # Step 3: Preview
    preview_config(config, detection_result)
    
    # Step 4: Test
    test_success = test_config(config)
    
    if not test_success:
        print("\n⚠️  Warning: Test scrape found 0 jobs. Config may need adjustment.")
    
    # Step 5: Confirm and save
    print("\n" + "=" * 60)
    confidence = detection_result.get('confidence', 'LOW')
    
    if confidence == 'HIGH' and test_success:
        print("Confidence: HIGH - Auto-accepting config")
        save_config(config)
        print(f"✓ Config saved: {config.source_id}.json")
    else:
        response = input("\nSave this configuration? [y/n/e] (e=edit): ").strip().lower()
        
        if response == 'y':
            save_config(config)
            print(f"✓ Config saved: {config.source_id}.json")
        elif response == 'e':
            print("\nTo edit the config, modify the JSON file after saving.")
            print(f"Config will be saved to: {get_configs_dir()}/{config.source_id}.json")
            save_config(config)
            print(f"✓ Config saved: {config.source_id}.json")
            print("\nYou can now edit the config file and test again.")
        else:
            print("Config not saved. Exiting.")
            sys.exit(0)
    
    print("\n" + "=" * 60)
    print("Next steps:")
    print(f"1. Test the scraper: python scripts/scrape.py {config.source_id}")
    print(f"2. Edit config if needed: backend/app/scrapers/configs/{config.source_id}.json")
    print(f"3. Register in registry if using config-based scrapers")
    print("=" * 60)


if __name__ == '__main__':
    main()

